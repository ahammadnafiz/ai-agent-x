{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896fa623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e5c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b5da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama3-8b-8192\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory=memory,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc39669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Ahammad Nafiz! I'm LLaMA, your friendly AI companion. I'm a large language model trained by a team of researcher at Meta AI. I've been trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of topics and questions. I'm excited to chat with you and help answer any questions you may have. What's on your mind today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, this is Ahammad Nafiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf00cbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice and easy one to start with! According to our conversation, your name is Ahammad Nafiz.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='What is my name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebcb9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, this is Ahammad Nafiz\n",
      "AI: Nice to meet you, Ahammad Nafiz! I'm LLaMA, your friendly AI companion. I'm a large language model trained by a team of researcher at Meta AI. I've been trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of topics and questions. I'm excited to chat with you and help answer any questions you may have. What's on your mind today?\n",
      "Human: What is my name?\n",
      "AI: Nice and easy one to start with! According to our conversation, your name is Ahammad Nafiz.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aaef63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, this is Ahammad Nafiz\\nAI: Nice to meet you, Ahammad Nafiz! I'm LLaMA, your friendly AI companion. I'm a large language model trained by a team of researcher at Meta AI. I've been trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of topics and questions. I'm excited to chat with you and help answer any questions you may have. What's on your mind today?\\nHuman: What is my name?\\nAI: Nice and easy one to start with! According to our conversation, your name is Ahammad Nafiz.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ac33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b97d29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35a84876",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f1424e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343565c",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3e5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ffd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59c4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4001090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b86357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"llama3-8b-8192\")\n",
    "memory = ConversationBufferWindowMemory(k=1)               \n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory=memory,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f07afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Ahammad Nafiz! I'm LLaMA, your friendly AI companion. I'm a large language model trained by a team of researcher at Meta AI. I've been trained on a massive dataset of text from the internet, which allows me to understand and respond to a wide range of topics and questions. I'm excited to chat with you and help answer any questions you may have. What's on your mind today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi, this is Ahammad Nafiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e1c83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahammad Nafiz! Unfortunately, I couldn\\'t find any notable or famous individuals by that name. It\\'s possible that Ahammad Nafiz is a private individual or not a public figure, which would explain why I couldn\\'t find any information about them.\\n\\nHowever, I can tell you that \"Ahammad\" is a common given name in many cultures, particularly in the Middle East and South Asia. It\\'s derived from the Arabic word \"Ahmad,\" which means \"praiseworthy.\" It\\'s also a common surname in some cultures.\\n\\nIf you\\'re looking for information about someone with that name, could you please provide more context or details about who Ahammad Nafiz is or what they\\'re known for? I\\'d be happy to try and help you find more information.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Tell me about Ahammad Nafiz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea7fa7",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bea25b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6652690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d4dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc69f0",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddd5c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43fc9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d70b92ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: Current summary: None\\n\\nNew lines of conversation:\\nHuman: Hello\\nAI: What\\'s up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\n\\nNew summary: The conversation starts with a casual greeting, with the human saying \"Hello\" and the AI responding with \"What\\'s up\". The human mentions they\\'re \"just hanging\" and the AI responds with \"Cool\". The human then asks about the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bb1b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54ef6b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The conversation starts with a casual greeting, with the human saying \"Hello\" and the AI responding with \"What\\'s up\". The human mentions they\\'re \"just hanging\" and the AI responds with \"Cool\". The human then asks about the schedule for the day. The AI provides a detailed schedule for the day, including a meeting at 8am with the product team, working on the LangChain project from 9am-12pm, and a lunch meeting at the Italian restaurant at noon. The human asks what would be a good demo to show during the lunch meeting.\\nAI: I\\'m glad you asked! For the LLM demo, I would recommend showcasing the text generation capabilities of our latest Large Language Model (LLM) architecture. You could generate a short story or a poem on the fly, and then have a conversation with the customer about the potential applications of this technology in their industry. Additionally, you could also demonstrate the model\\'s ability to summarize long documents or articles, and how it can be used for tasks such as research summarization or document analysis.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4454cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: Current summary: None\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "\n",
      "New summary: The conversation starts with a casual greeting, with the human saying \"Hello\" and the AI responding with \"What's up\". The human mentions they're \"just hanging\" and the AI responds with \"Cool\". The human then asks about the schedule for the day.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm glad you asked! For the LLM demo, I would recommend showcasing the text generation capabilities of our latest Large Language Model (LLM) architecture. You could generate a short story or a poem on the fly, and then have a conversation with the customer about the potential applications of this technology in their industry. Additionally, you could also demonstrate the model's ability to summarize long documents or articles, and how it can be used for tasks such as research summarization or document analysis.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
