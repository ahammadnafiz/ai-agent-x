{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown \"https://drive.google.com/uc?id=1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE\" -O pdfs/yolov7paper.pdf\n",
    "# !gdown \"https://drive.google.com/uc?id=1vILwiv6nS2wI3chxNabMgry3qnV67TxM\" -O pdfs/rachelgreecv.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Text from the PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Extracted Data into Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22425/609336717.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
      "2025-03-24 19:48:33.997446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742824114.022978   22425 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742824114.029431   22425 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742824114.049996   22425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742824114.050035   22425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742824114.050037   22425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742824114.050040   22425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-24 19:48:34.058280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5099515318870544, 0.3396693468093872, 0.018211059272289276, -0.3898174464702606, -0.05946885421872139, -0.12047633528709412, 0.07061275094747543, 0.04617594555020332, 0.4419732987880707, 0.029263949021697044, -0.005520263686776161, 0.15431848168373108, 0.22259627282619476, -0.08958163857460022, -0.08633971959352493, -0.29083117842674255, 0.011318222619593143, -0.1661413311958313, -0.40186846256256104, 0.14409500360488892, 0.09455043822526932, 0.22527383267879486, 0.633211076259613, -0.08051353693008423, 0.0694948360323906, -0.2596285343170166, 0.3071894347667694, 0.29075026512145996, -0.10931214690208435, 0.14082826673984528, -0.1997317373752594, 0.43527019023895264, -0.011179097928106785, -0.04854302108287811, -0.5683408379554749, 0.6682410836219788, 0.5142979025840759, -0.04513942077755928, -0.2503037750720978, 0.265929251909256, 0.03157723322510719, 0.1909891664981842, 0.17394207417964935, 0.12888038158416748, 0.48200905323028564, -0.026408813893795013, -0.22784410417079926, -0.45952141284942627, -0.12979842722415924, 0.428998738527298, 0.0670669674873352, -0.18654035031795502, 0.2731333076953888, -0.21830858290195465, 0.3108298182487488, 0.20538447797298431, 0.2776709198951721, 0.237366184592247, 0.1178107038140297, 0.10687928646802902, 0.16285012662410736, -0.0990288034081459, -0.03519599512219429, 0.6454840898513794, 0.4266178011894226, -0.15947702527046204, -0.10718891769647598, -0.03984894976019859, -0.30119431018829346, -0.038517896085977554, 0.09996390342712402, 0.10715889930725098, 0.26384398341178894, 0.22231322526931763, -0.15319308638572693, -0.19680634140968323, -0.10447566211223602, -0.5703985691070557, 0.22217248380184174, -0.08743701875209808, 0.24527429044246674, -0.1801556944847107, -0.1588914394378662, 0.2682046592235565, -0.07089719921350479, 0.14886070787906647, 0.44496527314186096, -0.09773769229650497, 0.4688219726085663, -0.20769444108009338, 0.19130009412765503, 0.4623102843761444, -0.00045781786320731044, -0.14975465834140778, -0.23861059546470642, -0.03299470990896225, 0.3837682008743286, -0.05923886224627495, -0.3022055923938751, 0.2929684817790985, -0.009002703242003918, -0.09139227867126465, 0.7868766784667969, -0.4695718586444855, 0.39011743664741516, -0.5625474452972412, -0.02757447399199009, -0.049577269703149796, -0.2236308455467224, -0.13024912774562836, -0.01971973292529583, -0.41666537523269653, -0.08195001631975174, 0.08648237586021423, 0.23979434370994568, -0.17555521428585052, 0.2255513072013855, 0.1526443064212799, -0.2025955617427826, -0.0359988771378994, 0.5679687261581421, 0.08866757899522781, -0.19462822377681732, -0.13575230538845062, -0.24751968681812286, 0.1505616307258606, -0.23663511872291565, 0.33021080493927, 0.14254356920719147, -0.1480342298746109, 0.05127529054880142, 0.1980477273464203, 0.07731538265943527, 0.47492295503616333, -0.0980261042714119, -0.3780844509601593, -0.13865496218204498, -0.12661337852478027, 0.06793390214443207, 0.23334823548793793, -0.21305890381336212, 0.3547687232494354, -0.24983735382556915, -0.3076869547367096, -0.009000441990792751, 0.19760456681251526, -0.03061455488204956, -0.20714305341243744, 0.17340220510959625, 0.36577466130256653, -0.1790085732936859, -0.30457034707069397, -0.0759323313832283, 0.4755352735519409, 0.3827248513698578, -0.21566304564476013, 0.5137264728546143, 0.33867189288139343, -0.27033746242523193, 0.07106717675924301, 0.18868683278560638, -0.2692568600177765, 0.04149999842047691, 0.5709924697875977, 0.067330501973629, -0.3814986050128937, -0.26651525497436523, -0.06031884253025055, 0.4292156398296356, 0.277972012758255, -0.001000916468910873, 0.2146064192056656, -0.0946887657046318, 0.36445921659469604, 0.09976925700902939, -0.23054371774196625, 0.4380875527858734, 0.28089192509651184, 0.12356429547071457, 0.04426383227109909, -0.19505715370178223, -0.162959486246109, -0.1517455130815506, 0.37986963987350464, -0.49870774149894714, -0.03923124447464943, 0.2558804154396057, -0.1836407333612442, 0.1795722395181656, -0.34888550639152527, -0.08265884965658188, -0.04012336581945419, 0.21728728711605072, 0.20879407227039337, -0.16286782920360565, 0.024954773485660553, 0.0632186159491539, -0.1144021674990654, -0.14589376747608185, -0.11089377105236053, -0.7712034583091736, 0.3173741400241852, 0.057605840265750885, -0.30654922127723694, -0.060510408133268356, -0.39408043026924133, -0.1166471317410469, 0.057929959148168564, -0.44710367918014526, -0.12994229793548584, -0.11991994827985764, -0.11536452174186707, -0.411594420671463, 0.2946013808250427, 0.5086432695388794, 0.3535793721675873, -0.14199501276016235, -0.06805171817541122, -0.13161087036132812, 0.16294598579406738, -0.34435996413230896, -0.633211076259613, -0.3039669096469879, -0.7938421368598938, -0.6371524333953857, -0.442533403635025, -0.5417777299880981, 0.26907238364219666, 0.2935393154621124, 0.31941816210746765, 0.01592012494802475, 0.3033120334148407, 0.18264837563037872, -0.3103834390640259, -0.08318370580673218, -0.650355875492096, -0.20516076683998108, -0.053103622049093246, 0.3602420687675476, -0.0510551892220974, -0.38664788007736206, 0.11299209296703339, -0.4679809510707855, 0.1254587322473526, -0.03653016313910484, 0.10451871901750565, -0.3266414999961853, 0.1912200003862381, 0.05032218620181084, 0.3571099638938904, 0.05628598853945732, 0.16348062455654144, 0.07613906264305115, 0.18797913193702698, 0.007443310227245092, -0.3070041835308075, -0.44210436940193176, 0.4063909649848938, 0.09221088886260986, 0.3720283806324005, 0.9323984384536743, -0.30221259593963623, -0.18829268217086792, -0.3523734509944916, -0.030842946842312813, -0.12233183532953262, -0.059098828583955765, 0.5025683045387268, -0.06436629593372345, 0.00873055774718523, -0.2036537081003189, 0.1577550321817398, 0.5096654891967773, -0.0711037889122963, -0.3316667079925537, -0.1526332050561905, 0.10225067287683487, -0.10038959980010986, -0.6112149357795715, 0.01654272899031639, -0.06585869938135147, -0.3347851037979126, -0.1306607872247696, -0.10921819508075714, -0.4801117777824402, -0.1997157633304596, -0.040688738226890564, 0.1404421329498291, -0.30724990367889404, 0.24554309248924255, 0.059313829988241196, -0.211147740483284, -0.2565946578979492, -0.15327559411525726, -0.18736839294433594, 0.21516241133213043, -0.18350528180599213, -0.4706684947013855, 0.4314775764942169, 0.30461692810058594, -0.37468621134757996, -0.30303093791007996, -0.10750250518321991, -0.3914441764354706, 0.45122984051704407, -0.3617643415927887, -0.30519112944602966, 0.4077059030532837, 0.2905380427837372, -0.4036751687526703, 0.025065245106816292, 0.45109206438064575, -0.006455688271671534, 0.5109851360321045, 0.011614813469350338, -0.018583228811621666, -0.8092647194862366, 0.4092475175857544, -0.18639610707759857, -0.128478541970253, 0.12251486629247665, 0.028154384344816208, 0.10663669556379318, -0.11087663471698761, -0.22662490606307983, 0.6112902164459229, 0.3892715573310852, -0.263494074344635, 0.4699702262878418, 0.5962346196174622, -0.14740577340126038, 0.16759763658046722, -0.9394888281822205, -0.36883604526519775, -0.2016996294260025, 0.2026318907737732, 0.09861614555120468, 0.08715075254440308, -0.42052945494651794, 0.0019231908954679966, -0.07605922967195511, 0.36308175325393677, -0.4049501419067383, -0.41116201877593994, 0.21086454391479492, 0.22639599442481995, -0.32482799887657166, 0.4431248903274536, -0.037921223789453506, -0.16219095885753632, -0.02933618612587452, 0.604445219039917, -0.5943877696990967, 0.13824309408664703, 0.021987324580550194, 0.15738321840763092, -0.030503343790769577, 0.3254673182964325, 0.3933248221874237, -0.1153317466378212, 0.626188337802887, -0.07758763432502747, -0.15981385111808777, 0.5335161685943604, 0.19136156141757965, -0.1121266782283783, 0.004889868199825287, -0.2814099192619324, -0.03717945143580437, 0.014244567602872849, -0.21029944717884064, -0.0700562372803688, 0.23693819344043732, 0.2480608969926834, 0.0060360729694366455, 0.10858368873596191, -0.02878519892692566, 0.132476806640625, -0.07008331269025803, 0.11096929758787155, 0.29288914799690247, -0.3467920124530792, 0.7267150282859802, 0.2378871589899063]\n"
     ]
    }
   ],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "results = hf_embeddings.embed_query(\"What is the YOLOv7 paper about?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GROQ_API_KEY=os.environ.get('GROQ_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY) # One time run\n",
    "\n",
    "index_name = \"test-index\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into Pinecone index. Just one time run it\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=hf_embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=hf_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acetaminophen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d942c7f6-c061-4888-8567-4c792e5e495d', metadata={'page': 1.0, 'source': 'pdfs/yolov7paper.pdf'}, page_content='label assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output'),\n",
       " Document(id='010d9a17-33b6-4d1a-a283-23c140058e5d', metadata={'page': 12.0, 'source': 'pdfs/yolov7paper.pdf'}, page_content='2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop, pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages'),\n",
       " Document(id='193bc47b-a623-4476-a039-079824937f5b', metadata={'page': 0.0, 'source': 'pdfs/yolov7paper.pdf'}, page_content='1\\narXiv:2207.02696v1  [cs.CV]  6 Jul 2022')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: formers for end-to-end object detection. In Proceedings of\n",
      "the International Conference on Learning Representations\n",
      "(ICLR), 2021. 10\n",
      "15\n",
      " Metadata: {'page': 14.0, 'source': 'pdfs/yolov7paper.pdf'}\n",
      " Similarity Score: 0.656794906\n",
      "\n",
      "Result 2: DETR, DINO-5scale-R50, ViT-Adapter-B and many other\n",
      "object detectors in speed and accuracy. Moreover, we train\n",
      "YOLOv7 only on MS COCO dataset from scratch without\n",
      "using any other datasets or pre-trained weights. Source\n",
      "code is released in https://github.com/WongKinYiu/yolov7.\n",
      "1. Introduction\n",
      "Real-time object detection is a very important topic in\n",
      "computer vision, as it is often a necessary component in\n",
      "computer vision systems. For example, multi-object track-\n",
      " Metadata: {'page': 0.0, 'source': 'pdfs/yolov7paper.pdf'}\n",
      " Similarity Score: 0.538994908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similarity search with scores\n",
    "query = \"What is Object Detection?\"\n",
    "results = docsearch.similarity_search_with_score(query, k=2)  # Retrieve top 2 most similar documents with scores\n",
    "\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\\n Metadata: {doc.metadata}\\n Similarity Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(api_key=GROQ_API_KEY, model_name=\"llama3-8b-8192\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22425/309444668.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(\"How does Object Detection work?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object detection is a computer vision technique that involves identifying and locating objects within an image or video. Here's a general overview of how it works:\n",
      "\n",
      "1. **Image Preprocessing**: The input image is preprocessed to enhance its quality and prepare it for object detection. This may include resizing, normalizing, and converting the image to a suitable format.\n",
      "2. **Feature Extraction**: A deep neural network is used to extract features from the input image. These features can be spatial (e.g., edges, corners) or semantic (e.g., object parts, textures).\n",
      "3. **Object Proposal Generation**: The network generates a set of object proposals, which are regions of the image that are likely to contain objects. These proposals are typically generated using algorithms such as region proposal networks (RPNs) or selective search.\n",
      "4. **Feature Integration**: The features extracted in step 2 are integrated with the object proposals to form a feature representation for each proposal. This is typically done using a technique such as spatial pyramid pooling or feature fusion.\n",
      "5. **Object Classification**: The feature representation for each proposal is passed through a classification layer to predict the class of the object (e.g., person, car, dog).\n",
      "6. **Object Localization**: The feature representation for each proposal is also passed through a regression layer to predict the bounding box coordinates (x, y, w, h) of the object.\n",
      "7. **Non-Maximum Suppression (NMS)**: The output of the object classification and localization steps is a set of detected objects, including their class labels and bounding box coordinates. NMS is used to eliminate overlapping detections and produce a final set of detected objects.\n",
      "\n",
      "Some common object detection architectures include:\n",
      "\n",
      "* YOLO (You Only Look Once): A real-time object detection system that uses a single neural network to predict object locations and classes.\n",
      "* FCOS (Full Convolutional One-Stage): A one-stage object detection system that uses a single neural network to predict object locations and classes.\n",
      "* SSD (Single Shot Detector): A real-time object detection system that uses a single neural network to predict object locations and classes.\n",
      "* Faster R-CNN (Region-based Convolutional Neural Networks): A two-stage object detection system that uses a region proposal network (RPN) to generate object proposals and a classification and regression network (CRN) to predict object classes and locations.\n",
      "\n",
      "These are just a few examples of the many object detection architectures that exist. Each architecture has its own strengths and weaknesses, and the choice of which one to use depends on the specific application and requirements.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # Use Pinecone as a retriever\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type( # Create a QA chain\n",
    "    llm=groq_llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "response = qa_chain.run(\"How does Object Detection work?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
